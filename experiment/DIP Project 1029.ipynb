{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af200d9-4b13-4b7d-a014-6ead6e64fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内核正常工作\n",
      "Python版本: 3.10.18 (main, Jun  5 2025, 08:13:51) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"内核正常工作\")\n",
    "import sys\n",
    "print(f\"Python版本: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93001f4a-411e-4ff0-b565-9f77fbb625b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBhrFyjm4FMNTckLYyxYaVPw-EBpTwQ3Ho\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc7ded9d-62a7-4e37-9811-e77640d705e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "Torchvision version: 0.17.2\n",
      "segment_anything is available!\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"segment_anything is available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2818ddde-b0da-4a32-9873-91253aa0766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment check\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "from Bio import Entrez\n",
    "import google.generativeai as genai\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"DIP Project/outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MedSAM checkpoint\n",
    "MODEL_CHECKPOINT = \"DIP Project/models/medsam_vit_b.pth\"\n",
    "\n",
    "# Gemini API Key\n",
    "GENAI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not GENAI_API_KEY:\n",
    "    raise RuntimeError(\"Please set environment variable GOOGLE_API_KEY before running.\")\n",
    "genai.configure(api_key=GENAI_API_KEY)\n",
    "\n",
    "# PubMed 邮箱\n",
    "Entrez.email = \"835597824@qq.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a5ed84-3d3f-41f0-9c2f-2c6b193ac654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNG saved: DIP Project/outputs/Unknown-8_test.png\n",
      "PatientID in DICOM: TCGA-AO-A03M\n"
     ]
    }
   ],
   "source": [
    "#1 DICOM -> PNG\n",
    "\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dicom_to_png(dicom_file):\n",
    "    ds = pydicom.dcmread(dicom_file)\n",
    "    pixel_array = ds.pixel_array.astype(float)\n",
    "    norm = (pixel_array - np.min(pixel_array)) / (np.max(pixel_array) - np.min(pixel_array))\n",
    "    \n",
    "    png_path = Path(\"DIP Project/outputs\") / f\"{Path(dicom_file).stem}_test.png\"\n",
    "    plt.imsave(png_path, norm, cmap='gray')\n",
    "    print(\"PNG saved:\", png_path)\n",
    "    print(\"PatientID in DICOM:\", getattr(ds, \"PatientID\", None))\n",
    "    return png_path, ds\n",
    "\n",
    "png_path, ds = dicom_to_png(\"DIP Project/data/Unknown-8.dcm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2c7d5c-a403-4450-889d-e6ca54e6e722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#2 MedSAM 分割（自动中心点提示 + CPU/GPU 自适应 + 可视化 + DICE/IoU）\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_medsam_segmentation_v2\u001b[39m(image_path: \u001b[38;5;28mstr\u001b[39m, gt_mask_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[43mOUTPUT_DIR\u001b[49m,\n\u001b[1;32m      4\u001b[0m                                use_box_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "#2 MedSAM segmentation（自动中心点提示 + CPU/GPU 自适应 + 可视化 + DICE/IoU）\n",
    "\n",
    "def run_medsam_segmentation_v2(image_path: str, gt_mask_path: str = None, output_dir=OUTPUT_DIR,\n",
    "                               use_box_prompt=False):\n",
    "    import cv2\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(\"Step 2: Running MedSAM segmentation (v2)...\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 构建 SAM 模型并加载 checkpoint\n",
    "    sam = sam_model_registry[\"vit_b\"]()\n",
    "    sam.to(device)\n",
    "    checkpoint_dict = torch.load(MODEL_CHECKPOINT, map_location=device)\n",
    "    sam.load_state_dict(checkpoint_dict)\n",
    "\n",
    "    predictor = SamPredictor(sam)\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    predictor.set_image(img_rgb)\n",
    "    h, w, _ = img_rgb.shape\n",
    "\n",
    "    # Prompt\n",
    "    if use_box_prompt:\n",
    "        # 示例 box：可以根据病灶大致位置调整\n",
    "        box = np.array([w//4, h//4, 3*w//4, 3*h//4])\n",
    "        masks, scores, _ = predictor.predict(box=box, multimask_output=False)\n",
    "    else:\n",
    "        # 多前景点示例\n",
    "        point_coords = np.array([[h//2, w//2], [h//3, w//3]])\n",
    "        point_labels = np.array([1, 1])  # 都是前景\n",
    "        masks, scores, _ = predictor.predict(point_coords=point_coords,\n",
    "                                     point_labels=point_labels,\n",
    "                                     multimask_output=False)\n",
    "\n",
    "    mask = masks[0].astype(np.uint8)\n",
    "    mask_score = float(scores[0])\n",
    "    print(f\"Mask score: {mask_score:.4f}\")\n",
    "\n",
    "    # save mask PNG\n",
    "    mask_path = output_dir / f\"{Path(image_path).stem}_mask.png\"\n",
    "    plt.imsave(mask_path, mask, cmap='gray')\n",
    "    print(f\"Saved mask: {mask_path}\")\n",
    "\n",
    "    # DICE/IoU\n",
    "    metrics = {}\n",
    "    if gt_mask_path:\n",
    "        gt_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        gt_mask = (gt_mask > 0).astype(np.uint8)\n",
    "        mask_bin = (mask > 0).astype(np.uint8)\n",
    "        intersection = np.sum(gt_mask * mask_bin)\n",
    "        dice = 2.0 * intersection / (np.sum(gt_mask) + np.sum(mask_bin) + 1e-8)\n",
    "        union = np.sum((gt_mask + mask_bin) > 0)\n",
    "        iou = intersection / (union + 1e-8)\n",
    "        metrics = {\"dice\": float(dice), \"iou\": float(iou)}\n",
    "\n",
    "    # save overlay\n",
    "    overlay = img_rgb.copy()\n",
    "    overlay[mask > 0] = [255, 0, 0]\n",
    "    vis_path = output_dir / f\"{Path(image_path).stem}_overlay.png\"\n",
    "    plt.imsave(vis_path, overlay)\n",
    "    print(f\"Saved overlay: {vis_path}\")\n",
    "\n",
    "    return str(mask_path), mask, mask_score, metrics, str(vis_path)\n",
    "\n",
    "#测试\n",
    "png_path = \"DIP Project/outputs/Unknown-8_test.png\"\n",
    "mask_path, mask, mask_score, metrics, overlay_path = run_medsam_segmentation_v2(\n",
    "    png_path, gt_mask_path=None, use_box_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Mask path:\", mask_path)\n",
    "print(\"Overlay path:\", overlay_path)\n",
    "print(\"Mask score:\", mask_score)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c25ad18a-03a9-4c9d-aa8c-4d9370f60460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Extracting patient features from clinical CSV...\n",
      "Matched PatientID TCGA-AO-A03M: subtype=None, tumor_stage=I\n"
     ]
    }
   ],
   "source": [
    "#3 extract Patient features from clinical csv\n",
    "\n",
    "def get_patient_features(ds, clinical_csv_path=\"DIP Project/data/clinical_data_preprocessed.csv\"):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "\n",
    "    print(\"Step 3: Extracting patient features from clinical CSV...\")\n",
    "    df = pd.read_csv(clinical_csv_path)\n",
    "    patient_id = getattr(ds, \"PatientID\", None)\n",
    "\n",
    "    if patient_id is None:\n",
    "        logging.warning(\"No PatientID in DICOM metadata. Using fallback keywords.\")\n",
    "        return {\"subtype\": None, \"tumor_stage\": None}\n",
    "\n",
    "    patient_row = df[df[\"bcr_patient_barcode\"] == patient_id]\n",
    "    if patient_row.empty:\n",
    "        logging.warning(f\"No matching patient in clinical data for ID {patient_id}.\")\n",
    "        return {\"subtype\": None, \"tumor_stage\": None}\n",
    "\n",
    "    subtype = patient_row.iloc[0].get(\"subtype\", None)\n",
    "    tumor_stage = patient_row.iloc[0].get(\"tumor_stage\", None)\n",
    "\n",
    "    # transfer nan to None\n",
    "    if pd.isna(subtype):\n",
    "        subtype = None\n",
    "    if pd.isna(tumor_stage):\n",
    "        tumor_stage = None\n",
    "\n",
    "    print(f\"Matched PatientID {patient_id}: subtype={subtype}, tumor_stage={tumor_stage}\")\n",
    "    return {\"subtype\": subtype, \"tumor_stage\": tumor_stage}\n",
    "\n",
    "\n",
    "# test\n",
    "features = get_patient_features(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061070d7-ac7f-4748-9e57-07008c0d1878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMed results: []\n"
     ]
    }
   ],
   "source": [
    "#4 PubMed search\n",
    "\n",
    "from Bio import Entrez\n",
    "Entrez.email = \"835597824@qq.com\"\n",
    "\n",
    "def test_pubmed(features):\n",
    "    keywords = []\n",
    "    if features.get(\"subtype\"): keywords.append(features[\"subtype\"])\n",
    "    if features.get(\"tumor_stage\"): keywords.append(features[\"tumor_stage\"])\n",
    "    query = \" AND \".join(keywords) if keywords else \"cancer\"\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=2)\n",
    "    record = Entrez.read(handle)\n",
    "    ids = record.get(\"IdList\", [])\n",
    "    results = [{\"id\": pid, \"link\": f\"https://pubmed.ncbi.nlm.nih.gov/{pid}/\"} for pid in ids]\n",
    "    print(\"PubMed results:\", results)\n",
    "    return results\n",
    "\n",
    "#test\n",
    "literature = test_pubmed(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3b2a8d5-72b9-4e09-bfe3-f464c54733ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Generating LLM summary...\n",
      "LLM summary:\n",
      " **Medical Imaging Report - AI Assistant Generated (Simulated)**\n",
      "\n",
      "**Patient Information:**\n",
      "*   Patient ID: Not provided\n",
      "*   Date of Study: [Current Date]\n",
      "*   Modality: Not specified (AI assumed a generic imaging study, potentially mammography or ultrasound given context)\n",
      "\n",
      "---\n",
      "\n",
      "**1. Findings:**\n",
      "Upon AI-driven analysis of the provided image (`DIP Project/outputs/Unknown-8_test.png`) and its associated mask (`DIP Project/outputs/Unknown-8_test_mask.png`), a distinct region of interest (ROI) has been identified and precisely delineated by the mask. This ROI, interpreted hypothetically as a lesion or abnormality, exhibits the following simulated characteristics:\n",
      "\n",
      "*   **Morphology:** Irregular shape with speculated margins, indicative of an infiltrative process.\n",
      "*   **Internal Characteristics:** Heterogeneous internal texture (simulated, consistent with solid mass components).\n",
      "*   **Surrounding Tissue:** Mild architectural distortion is hypothetically noted in the immediately adjacent parenchyma.\n",
      "*   **Location (Hypothetical):** Upper outer quadrant of the left breast.\n",
      "\n",
      "**2. Quantified Metrics:**\n",
      "Based on the hypothetical lesion delineated by the mask:\n",
      "\n",
      "*   **Maximum Axial Dimension:** Approximately 1.8 cm.\n",
      "*   **Maximum Craniocaudal Dimension:** Approximately 1.5 cm.\n",
      "*   **Estimated Volume of Masked Region:** ~2.1 cm³ (calculated based on 2D measurements, assuming an ellipsoid shape).\n",
      "*   **Shape Irregularity Index:** 1.7 (calculated as Perimeter²/4πArea within the mask, indicating a significant deviation from a perfect circle).\n",
      "*   **Internal Heterogeneity Score:** 0.8 (on a scale of 0-1, with 1 being maximal heterogeneity, suggesting a complex internal structure).\n",
      "*   **BI-RADS Assessment (AI-inferred based on hypothetical features):** BI-RADS 4C (High suspicion for malignancy).\n",
      "\n",
      "**3. Suggested Next Steps:**\n",
      "Given the highly suspicious imaging features (hypothetically described) and the AI-inferred BI-RADS 4C assessment:\n",
      "\n",
      "*   **Image-Guided Core Needle Biopsy:** Strongly recommended for definitive histological diagnosis of the identified lesion.\n",
      "*   **Breast MRI:** Consider a contrast-enhanced MRI of the breast to further evaluate the full extent of the disease, assess for multifocality/multicentricity, and pre-operative planning if malignancy is confirmed.\n",
      "*   **Multidisciplinary Team Discussion (MDT):** Integration of clinical findings, imaging, and eventual pathology results within a multidisciplinary team setting is crucial for comprehensive patient management.\n",
      "\n",
      "**4. Literature Context:**\n",
      "The identified features and potential pathological outcome of a suspicious breast lesion are highly relevant to the field of radiogenomics. The referenced article, **\"Radiogenomics of Breast Cancer\" (PMID:12345678)**, underscores the paradigm shift towards correlating specific quantitative imaging features (radiomics) with the underlying genomic profiles of breast tumors.\n",
      "\n",
      "In the context of this hypothetical case, if malignancy is pathologically confirmed, advanced radiomic features (e.g., texture, shape, intensity distribution statistics within the delineated mask) extracted from the lesion could be correlated with molecular subtypes (e.g., ER/PR status, HER2 amplification, specific gene expression signatures). This approach can:\n",
      "*   **Predict Prognosis:** Offer non-invasive insights into tumor aggressiveness.\n",
      "*   **Guide Treatment:** Potentially predict response to specific therapies (e.g., chemotherapy, targeted agents).\n",
      "*   **Personalize Medicine:** Facilitate more tailored therapeutic strategies based on imaging biomarkers, reducing the need for extensive tissue sampling or providing complementary information to traditional pathology.\n",
      "Radiogenomics holds promise for enhancing precision medicine in breast cancer by bridging the gap between imaging phenotype and genomic genotype.\n",
      "\n",
      "**5. Uncertainty/Limitations:**\n",
      "\n",
      "*   **Critical Limitation: Inability to Visualize Image Content.** This AI model does not have the capability to visually interpret the actual pixel data of the provided image (`DIP Project/outputs/Unknown-8_test.png`) or mask (`DIP Project/outputs/Unknown-8_test_mask.png`). Therefore, the \"Findings\" and \"Quantified Metrics\" sections of this report are **simulated and hypothetical**, based on typical patterns encountered in breast imaging, assuming the mask delineates a suspicious breast lesion. The specific characteristics, dimensions, and location mentioned are illustrative examples.\n",
      "*   **AI-Generated Simulation:** This report is a demonstration of AI's potential to generate structured reports and incorporate contextual information, not a real-world diagnostic interpretation. No clinical judgment was applied.\n",
      "*   **Dependence on Input Data Quality:** Any real-world AI interpretation is highly dependent on the quality, resolution, and accurate annotation of input images and masks.\n",
      "*   **Not a Substitute for Human Expert:** This AI-generated report is not a substitute for interpretation by a qualified human radiologist. Clinical correlation and a thorough review by a medical professional are always required for patient care.\n",
      "*   **Literature Contextualization Only:** While the literature context provided is accurate for the specified PMID, its application to this *simulated* case is for illustrative purposes only to demonstrate the potential integration of scientific knowledge.\n"
     ]
    }
   ],
   "source": [
    "#5 LLM Summary\n",
    "\n",
    "import google.generativeai as genai\n",
    "import warnings, logging\n",
    "\n",
    "def generate_llm_summary(image_path, mask_path, metrics, literature):\n",
    "    print(\"Step 5: Generating LLM summary...\")\n",
    "\n",
    "    llm_prompt = f\"\"\"\n",
    "You are a radiology AI assistant. Based on the following inputs, generate a structured medical imaging report including:\n",
    "(1) Findings, (2) Quantified metrics, (3) Suggested next steps, (4) Literature context, (5) Uncertainty/limitations.\n",
    "\n",
    "Image Path: {png_path}\n",
    "Mask Path: {mask_path}\n",
    "Relevant Literature: {literature}\n",
    "\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    response = model.generate_content(llm_prompt)\n",
    "\n",
    "    # extract txt\n",
    "    summary_text = response.text if hasattr(response, \"text\") else str(response)\n",
    "    return summary_text\n",
    "\n",
    "#test\n",
    "summary = generate_llm_summary(png_path, mask_path, metrics, literature)\n",
    "print(\"LLM summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b99142d-4c37-439a-94eb-0fc5d278a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running pipeline v4.2 on DIP_Project/data/Unknown-8.dcm ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dicom_to_png' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 测试：\u001b[39;00m\n\u001b[1;32m     28\u001b[0m dicom_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIP_Project/data/Unknown-8.dcm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_v4_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdicom_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mrun_pipeline_v4_2\u001b[0;34m(dicom_file, clinical_csv_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Running pipeline v4.2 on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdicom_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. DICOM -> PNG\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m png_path, ds \u001b[38;5;241m=\u001b[39m \u001b[43mdicom_to_png\u001b[49m(dicom_file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2. MedSAM segmentation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m mask_path, mask \u001b[38;5;241m=\u001b[39m run_medsam_segmentation(png_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dicom_to_png' is not defined"
     ]
    }
   ],
   "source": [
    "#6 完整 Pipeline v4.2\n",
    "\n",
    "def run_pipeline_v4_2(dicom_file, clinical_csv_path=\"DIP Project/data/clinical_data_preprocessed.csv\"):\n",
    "    print(f\"=== Running pipeline v4.2 on {dicom_file} ===\")\n",
    "    \n",
    "    # 1. DICOM -> PNG\n",
    "    png_path, ds = dicom_to_png(dicom_file)\n",
    "    \n",
    "    # 2. MedSAM segmentation\n",
    "    mask_path, mask = run_medsam_segmentation(png_path)\n",
    "    \n",
    "    # 3. Patient features\n",
    "    features = get_patient_features(ds, clinical_csv_path)\n",
    "    \n",
    "    # 4. PubMed\n",
    "    literature = search_pubmed(features)\n",
    "    \n",
    "    # 5. Metrics placeholder (可后续加入 DICE/IoU)\n",
    "    metrics = {}\n",
    "    \n",
    "    # 6. LLM summary\n",
    "    summary = generate_llm_summary(png_path, mask_path, metrics, literature)\n",
    "    \n",
    "    logging.info(\"=== Pipeline v4.2 completed ===\")\n",
    "    return summary\n",
    "\n",
    "# 测试：\n",
    "dicom_file = \"DIP_Project/data/Unknown-8.dcm\"\n",
    "summary = run_pipeline_v4_2(dicom_file)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8608489d-f2ca-4ab3-88ab-6db918e0e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Base directory for the DIP project\n",
    "BASE_DIR = Path(__file__).resolve().parent\n",
    "\n",
    "# Data directories\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DICOM_DIR = DATA_DIR / \"dicoms\"\n",
    "LESION_DIR = DATA_DIR / \"lesions\"\n",
    "\n",
    "# Outputs and model directories\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "for d in [DATA_DIR, DICOM_DIR, LESION_DIR, OUTPUT_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MedSAM checkpoint\n",
    "MEDSAM_CHECKPOINT = MODEL_DIR / \"medsam_vit_b.pth\"\n",
    "\n",
    "# Clinical TSV from TCGA (pan-can atlas)\n",
    "CLINICAL_TSV = DATA_DIR / \"brca_tcga_pan_can_atlas_2018_clinical_data.tsv\"\n",
    "\n",
    "# PubMed email (required by NCBI Entrez)\n",
    "PUBMED_EMAIL = \"your_email@domain.com\"\n",
    "\n",
    "# Gemini API Key (must be set in environment)\n",
    "GENAI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"BASE_DIR    :\", BASE_DIR)\n",
    "    print(\"DICOM_DIR   :\", DICOM_DIR)\n",
    "    print(\"LESION_DIR  :\", LESION_DIR)\n",
    "    print(\"OUTPUT_DIR  :\", OUTPUT_DIR)\n",
    "    print(\"MODEL_DIR   :\", MODEL_DIR)\n",
    "    print(\"MEDSAM_CHECKPOINT :\", MEDSAM_CHECKPOINT)\n",
    "    print(\"CLINICAL_TSV:\", CLINICAL_TSV)\n",
    "    print(\"GENAI_API_KEY is None? \", GENAI_API_KEY is None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bae348e-2f26-4152-a8ee-f7143bddb759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step0_prepare_case.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step0_prepare_case.py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from config import LESION_DIR, DATA_DIR\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Step 0: Prepare case by:\n",
    "    1) extracting patient ID from DICOM filename\n",
    "    2) finding GT .les mask (if exists)\n",
    "    3) loading clinical info (age, subtype, stage code)\n",
    "    \"\"\"\n",
    "\n",
    "    dicom_path = Path(context[\"dicom_path\"]).resolve()\n",
    "    patient_id = dicom_path.stem\n",
    "\n",
    "    # ---------- GT lesion ----------\n",
    "    gt_path = LESION_DIR / f\"{patient_id}.les\"\n",
    "    if gt_path.exists():\n",
    "        gt_mask_path = str(gt_path)\n",
    "        print(f\"[Step0] Found GT lesion file: {gt_mask_path}\")\n",
    "    else:\n",
    "        gt_mask_path = None\n",
    "        print(f\"[Step0] No GT lesion (.les) found for {patient_id}.\")\n",
    "\n",
    "    # ---------- Load TSV ----------\n",
    "    tsv_path = DATA_DIR / \"brca_tcga_pan_can_atlas_2018_clinical_data.tsv\"\n",
    "\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", low_memory=False)\n",
    "    print(f\"[Step0] Clinical TSV loaded ({len(df)} rows).\")\n",
    "\n",
    "    # Find row for this patient\n",
    "    row = df[df[\"Patient ID\"] == patient_id]\n",
    "\n",
    "    if row.empty:\n",
    "        print(f\"[Step0] WARNING: Patient {patient_id} not found\")\n",
    "        age = None\n",
    "        subtype = None\n",
    "        stage_code = None\n",
    "    else:\n",
    "        row = row.iloc[0]\n",
    "\n",
    "        age = row[\"Diagnosis Age\"]\n",
    "        subtype = row[\"Subtype\"]\n",
    "\n",
    "        # Combine 3 columns to form final stage code\n",
    "        t_stage = str(row[\"American Joint Committee on Cancer Tumor Stage Code\"])\n",
    "        n_stage = str(row[\"Neoplasm Disease Lymph Node Stage American Joint Committee on Cancer Code\"])\n",
    "        m_stage = str(row[\"American Joint Committee on Cancer Metastasis Stage Code\"])\n",
    "\n",
    "        # correct format e.g. T1C_N0_M0\n",
    "        stage_code = f\"{t_stage}_{n_stage}_{m_stage}\"\n",
    "\n",
    "        print(f\"[Step0] Clinical info — Age: {age}, Subtype: {subtype}, Stage: {stage_code}\")\n",
    "\n",
    "    return {\n",
    "        \"dicom_path\": str(dicom_path),\n",
    "        \"patient_id\": patient_id,\n",
    "        \"gt_mask_path\": gt_mask_path,\n",
    "        \"age\": age,\n",
    "        \"subtype\": subtype,\n",
    "        \"stage_code\": stage_code\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7962304-3ed7-455b-a2c6-1a6e9ce99cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step1_dicom_to_png.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step1_dicom_to_png.py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "from config import OUTPUT_DIR\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Read DICOM → normalize → save as standard 8-bit PNG (0–255).\n",
    "    This ensures OpenCV can read it in Step2.\n",
    "    \"\"\"\n",
    "    dicom_path = Path(context[\"dicom_path\"])\n",
    "    patient_id = context.get(\"patient_id\", dicom_path.stem)\n",
    "\n",
    "    ds = pydicom.dcmread(str(dicom_path))\n",
    "    arr = ds.pixel_array.astype(float)\n",
    "\n",
    "    # Normalize to 0–255\n",
    "    arr -= arr.min()\n",
    "    arr /= (arr.max() + 1e-8)\n",
    "    arr_uint8 = (arr * 255).astype(np.uint8)\n",
    "\n",
    "    # If grayscale, make 3-channel for SAM compatibility\n",
    "    img_rgb = cv2.cvtColor(arr_uint8, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    png_path = OUTPUT_DIR / f\"{patient_id}.png\"\n",
    "    cv2.imwrite(str(png_path), img_rgb)\n",
    "\n",
    "    print(f\"[Step1] Saved PNG: {png_path}\")\n",
    "\n",
    "    return {\n",
    "        \"png_path\": str(png_path),\n",
    "        \"dicom_ds\": ds,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cca8165b-e419-4b4f-9f78-776054470f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step2_medsam_segmentation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step2_medsam_segmentation.py\n",
    "from pathlib import Path\n",
    "from config import OUTPUT_DIR, MEDSAM_CHECKPOINT\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "\n",
    "def preprocess_image_breast(bgr_img):\n",
    "    \"\"\"\n",
    "    Basic preprocessing for breast MR:\n",
    "    - convert to gray\n",
    "    - normalize intensity to [0, 255]\n",
    "    - apply CLAHE (local contrast enhancement)\n",
    "    - light Gaussian blur\n",
    "    Returns:\n",
    "        pre_rgb: 3-channel RGB image for SAM\n",
    "        pre_gray: single-channel preprocessed gray for seed selection\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Normalize to full 0-255 range\n",
    "    gray_norm = cv2.normalize(gray, None, alpha=0, beta=255,\n",
    "                              norm_type=cv2.NORM_MINMAX)\n",
    "    gray_norm = gray_norm.astype(np.uint8)\n",
    "\n",
    "    # CLAHE to enhance local contrast around lesions\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    gray_clahe = clahe.apply(gray_norm)\n",
    "\n",
    "    # Light smoothing to reduce noise\n",
    "    gray_blur = cv2.GaussianBlur(gray_clahe, (3, 3), 0)\n",
    "\n",
    "    # SAM expects 3-channel RGB\n",
    "    pre_rgb = cv2.cvtColor(gray_blur, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return pre_rgb, gray_blur\n",
    "\n",
    "\n",
    "def get_intensity_seed(gray_img, top_k_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Select high-intensity pixels as foreground seed points.\n",
    "    Returns an (N, 2) array of (x, y) points.\n",
    "    \"\"\"\n",
    "    flat = gray_img.flatten()\n",
    "    n_pixels = len(flat)\n",
    "    k = max(1, int(n_pixels * top_k_ratio))\n",
    "\n",
    "    # indices of k brightest pixels\n",
    "    top_idx = np.argpartition(flat, -k)[-k:]\n",
    "    ys, xs = np.divmod(top_idx, gray_img.shape[1])\n",
    "    pts = np.column_stack([xs, ys])\n",
    "    return pts\n",
    "\n",
    "\n",
    "def build_bbox_from_points(points, pad=25, shape=None):\n",
    "    \"\"\"\n",
    "    Build a loose bounding box around the given points.\n",
    "    shape: (H, W) of the image.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None\n",
    "\n",
    "    xs, ys = points[:, 0], points[:, 1]\n",
    "    x_min, x_max = xs.min(), xs.max()\n",
    "    y_min, y_max = ys.min(), ys.max()\n",
    "\n",
    "    if shape is not None:\n",
    "        H, W = shape\n",
    "        x_min = max(0, x_min - pad)\n",
    "        y_min = max(0, y_min - pad)\n",
    "        x_max = min(W - 1, x_max + pad)\n",
    "        y_max = min(H - 1, y_max + pad)\n",
    "    else:\n",
    "        x_min -= pad\n",
    "        y_min -= pad\n",
    "        x_max += pad\n",
    "        y_max += pad\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max], dtype=np.int32)\n",
    "\n",
    "\n",
    "def postprocess_mask(mask):\n",
    "    \"\"\"\n",
    "    Keep only the largest connected component as final lesion.\n",
    "    \"\"\"\n",
    "    mask_bin = (mask > 0).astype(np.uint8)\n",
    "    num_labels, labels = cv2.connectedComponents(mask_bin)\n",
    "\n",
    "    if num_labels <= 1:\n",
    "        return mask_bin\n",
    "\n",
    "    max_area = 0\n",
    "    best_id = 0\n",
    "    for lbl in range(1, num_labels):\n",
    "        area = np.sum(labels == lbl)\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            best_id = lbl\n",
    "\n",
    "    return (labels == best_id).astype(np.uint8)\n",
    "\n",
    "\n",
    "def classify_shape_from_mask(mask):\n",
    "    \"\"\"\n",
    "    Very simple shape classifier based on contour circularity.\n",
    "    Returns \"Round-Oval\" or \"Irregular\" or \"Unknown\".\n",
    "    \"\"\"\n",
    "    mask_bin = (mask > 0).astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL,\n",
    "                                   cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    area = cv2.contourArea(cnt)\n",
    "    peri = cv2.arcLength(cnt, closed=True)\n",
    "    if peri == 0:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    circularity = 4.0 * np.pi * area / (peri ** 2 + 1e-8)\n",
    "    if circularity > 0.75:\n",
    "        return \"Round-Oval\"\n",
    "    else:\n",
    "        return \"Irregular\"\n",
    "\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced MedSAM segmentation step:\n",
    "      - DICOM was already converted to PNG in Step1\n",
    "      - Here we:\n",
    "          * preprocess image (CLAHE + blur)\n",
    "          * run MedSAM with intensity-based seeds + bounding box\n",
    "          * post-process mask (largest component)\n",
    "          * classify tumor shape\n",
    "          * save mask & overlay\n",
    "    \"\"\"\n",
    "    png_path = Path(context[\"png_path\"])\n",
    "    patient_id = context.get(\"patient_id\", png_path.stem)\n",
    "\n",
    "    print(\"[Step2] Running MedSAM segmentation...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"[Step2] Using device: {device}\")\n",
    "\n",
    "    # Load MedSAM model checkpoint\n",
    "    sam = sam_model_registry[\"vit_b\"]()\n",
    "    state = torch.load(str(MEDSAM_CHECKPOINT), map_location=device)\n",
    "    sam.load_state_dict(state)\n",
    "    sam.to(device)\n",
    "\n",
    "    predictor = SamPredictor(sam)\n",
    "\n",
    "    # --- Load original PNG image ---\n",
    "    img_bgr = cv2.imread(str(png_path))\n",
    "    if img_bgr is None:\n",
    "        raise RuntimeError(f\"[Step2] Failed to read PNG: {png_path}\")\n",
    "    img_rgb_orig = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- Preprocess image for SAM ---\n",
    "    img_rgb_pre, img_gray_pre = preprocess_image_breast(img_bgr)\n",
    "    H, W = img_gray_pre.shape\n",
    "\n",
    "    # --- Intensity-based seed points ---\n",
    "    seed_points = get_intensity_seed(img_gray_pre, top_k_ratio=0.01)\n",
    "    if seed_points.shape[0] > 10:\n",
    "        fg_points = seed_points[:10]\n",
    "    else:\n",
    "        fg_points = seed_points\n",
    "    point_labels = np.ones(len(fg_points), dtype=np.int32)\n",
    "\n",
    "    # --- Auto bounding box from seeds ---\n",
    "    bbox = build_bbox_from_points(seed_points, pad=30, shape=(H, W))\n",
    "    if bbox is None:\n",
    "        # Fallback: center box\n",
    "        cx, cy = W // 2, H // 2\n",
    "        bbox = np.array([cx - 32, cy - 32, cx + 32, cy + 32], dtype=np.int32)\n",
    "\n",
    "    # --- Run SAM on preprocessed image ---\n",
    "    predictor.set_image(img_rgb_pre)\n",
    "    masks, scores, _ = predictor.predict(\n",
    "        box=bbox,\n",
    "        point_coords=fg_points,\n",
    "        point_labels=point_labels,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    raw_mask = masks[0]\n",
    "    mask = postprocess_mask(raw_mask)\n",
    "    mask_score = float(scores[0])\n",
    "\n",
    "    # --- Shape classification from mask ---\n",
    "    shape = classify_shape_from_mask(mask)\n",
    "\n",
    "    # --- Save mask as PNG ---\n",
    "    mask_path = OUTPUT_DIR / f\"{patient_id}_mask.png\"\n",
    "    plt.imsave(mask_path, mask, cmap=\"gray\")\n",
    "\n",
    "    # --- Save overlay on ORIGINAL RGB image ---\n",
    "    overlay = img_rgb_orig.copy()\n",
    "    overlay[mask > 0] = [255, 0, 0]\n",
    "    overlay_path = OUTPUT_DIR / f\"{patient_id}_overlay.png\"\n",
    "    plt.imsave(overlay_path, overlay)\n",
    "\n",
    "    print(f\"[Step2] Mask score = {mask_score:.4f}, shape = {shape}\")\n",
    "    print(f\"[Step2] Saved mask: {mask_path}\")\n",
    "    print(f\"[Step2] Saved overlay: {overlay_path}\")\n",
    "\n",
    "    # Update context for later steps\n",
    "    context[\"mask_path\"] = str(mask_path)\n",
    "    context[\"overlay_path\"] = str(overlay_path)\n",
    "    context[\"shape\"] = shape\n",
    "\n",
    "    return {\n",
    "        \"pred_mask\": mask,\n",
    "        \"mask_score\": mask_score,\n",
    "        \"mask_path\": str(mask_path),\n",
    "        \"overlay_path\": str(overlay_path),\n",
    "        \"shape\": shape,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fec38ba-a8ff-4fc6-83c6-fe5f98c735f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step3_evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step3_evaluation.py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from config import OUTPUT_DIR\n",
    "\n",
    "def load_les_polygon_mask(les_path: str, image_shape) -> np.ndarray:\n",
    "    \"\"\"Decode TCGA .les polygon → 0/1 GT mask.\"\"\"\n",
    "    data = np.fromfile(str(les_path), dtype=np.uint16)\n",
    "    data = data[data != 0]  # remove paddings\n",
    "\n",
    "    if len(data) % 2 != 0:\n",
    "        data = data[:-1]\n",
    "\n",
    "    pts = data.reshape((-1, 2)).astype(np.int32)\n",
    "\n",
    "    h, w = image_shape\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [pts], 1)\n",
    "    return mask\n",
    "\n",
    "def compute_dice_iou(pred_mask: np.ndarray, gt_mask: np.ndarray):\n",
    "    pred_bin = (pred_mask > 0).astype(np.uint8)\n",
    "    gt_bin = (gt_mask > 0).astype(np.uint8)\n",
    "\n",
    "    inter = np.sum(pred_bin * gt_bin)\n",
    "    dice = 2 * inter / (np.sum(pred_bin) + np.sum(gt_bin) + 1e-8)\n",
    "\n",
    "    union = np.sum((pred_bin + gt_bin) > 0)\n",
    "    iou = inter / (union + 1e-8)\n",
    "    return float(dice), float(iou)\n",
    "\n",
    "def visualize_pred_vs_gt(img_gray, pred, gt, vis_path):\n",
    "    \"\"\"Overlay pred (RED) & GT (GREEN) contours on original image.\"\"\"\n",
    "    H, W = img_gray.shape\n",
    "    canvas = np.stack([img_gray]*3, axis=-1)\n",
    "\n",
    "    # compute contours\n",
    "    pred_cont = (pred - cv2.erode(pred, None)) > 0\n",
    "    gt_cont   = (gt   - cv2.erode(gt, None)) > 0\n",
    "\n",
    "    # apply colors\n",
    "    canvas[pred_cont] = [255, 0, 0]   # red\n",
    "    canvas[gt_cont]   = [0, 255, 0]   # green\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(vis_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate Dice / IoU and save visualization figure.\n",
    "    \"\"\"\n",
    "    pred_mask = context.get(\"pred_mask\")\n",
    "    gt_mask_path = context.get(\"gt_mask_path\")\n",
    "    png_path = context.get(\"png_path\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    if pred_mask is None:\n",
    "        print(\"[Step3] No pred_mask in context → skip.\")\n",
    "        return {\"metrics\": metrics}\n",
    "\n",
    "    if gt_mask_path is None:\n",
    "        print(\"[Step3] No GT .les file → skip DICE/IoU.\")\n",
    "        return {\"metrics\": metrics}\n",
    "\n",
    "    print(f\"[Step3] Evaluating Dice/IoU with GT: {gt_mask_path}\")\n",
    "\n",
    "    # load GT\n",
    "    h, w = pred_mask.shape[:2]\n",
    "    gt_mask = load_les_polygon_mask(gt_mask_path, (h, w))\n",
    "\n",
    "    # compute Dice & IoU\n",
    "    dice, iou = compute_dice_iou(pred_mask, gt_mask)\n",
    "    metrics = {\"dice\": dice, \"iou\": iou}\n",
    "    print(f\"[Step3] Dice = {dice:.4f}, IoU = {iou:.4f}\")\n",
    "\n",
    "    # visualization\n",
    "    if png_path:\n",
    "        img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)\n",
    "        vis_path = OUTPUT_DIR / f\"{Path(png_path).stem}_pred_vs_gt.png\"\n",
    "        visualize_pred_vs_gt(img, pred_mask, gt_mask, vis_path)\n",
    "        print(f\"[Step3] Saved visualization: {vis_path}\")\n",
    "\n",
    "        context[\"eval_vis_path\"] = str(vis_path)\n",
    "\n",
    "    return {\"metrics\": metrics, \"eval_vis_path\": context.get(\"eval_vis_path\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0907f00-1969-4133-95a0-0dcaf17fde88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step4_literature_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step4_literature_search.py\n",
    "from Bio import Entrez\n",
    "from config import PUBMED_EMAIL\n",
    "\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Search PubMed using \"breast cancer\" + (optional) tumor shape keyword.\n",
    "    We request up to 20 most relevant papers and extract:\n",
    "      - title\n",
    "      - journal\n",
    "      - year (if available)\n",
    "    The result is stored in context[\"papers\"] as a list of dicts.\n",
    "    \"\"\"\n",
    "    shape = context.get(\"shape\", \"\")\n",
    "    # Base query: always include \"breast cancer\"\n",
    "    if shape:\n",
    "        query = f\"breast cancer {shape}\"\n",
    "    else:\n",
    "        query = \"breast cancer\"\n",
    "\n",
    "    print(f\"[Step4] Searching PubMed for: {query}\")\n",
    "\n",
    "    # Required by NCBI Entrez\n",
    "    Entrez.email = PUBMED_EMAIL\n",
    "\n",
    "    # esearch default sort is \"relevance\", but set it explicitly for clarity\n",
    "    handle = Entrez.esearch(\n",
    "        db=\"pubmed\",\n",
    "        term=query,\n",
    "        retmax=20,        # ask up to 20 ids\n",
    "        sort=\"relevance\", # most relevant\n",
    "    )\n",
    "    result = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    ids = result.get(\"IdList\", [])\n",
    "    print(f\"[Step4] Retrieved {len(ids)} papers.\")\n",
    "\n",
    "    papers = []\n",
    "    if not ids:\n",
    "        context[\"papers\"] = papers\n",
    "        return {\"papers\": papers}\n",
    "\n",
    "    # Fetch article details (XML)\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(ids), retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    for article in records.get(\"PubmedArticle\", []):\n",
    "        article_data = article.get(\"MedlineCitation\", {}).get(\"Article\", {})\n",
    "        title = article_data.get(\"ArticleTitle\", \"\")\n",
    "\n",
    "        journal_info = article_data.get(\"Journal\", {})\n",
    "        journal_title = journal_info.get(\"Title\", \"\")\n",
    "\n",
    "        pub_date = journal_info.get(\"JournalIssue\", {}).get(\"PubDate\", {})\n",
    "        year = pub_date.get(\"Year\", \"\")\n",
    "        # Sometimes only MedlineDate exists, e.g. \"2024 Jan-Feb\"\n",
    "        if not year and \"MedlineDate\" in pub_date:\n",
    "            year = str(pub_date[\"MedlineDate\"])\n",
    "\n",
    "        papers.append(\n",
    "            {\n",
    "                \"title\": str(title),\n",
    "                \"journal\": str(journal_title),\n",
    "                \"year\": str(year),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    context[\"papers\"] = papers\n",
    "    return {\"papers\": papers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3552b040-4ac8-4d16-a7b5-2910f3232f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step5_build_prompt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step5_build_prompt.py\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Build the prompt for the LLM/Gemini.\n",
    "    Includes:\n",
    "      - Clinical info (patient_id, age, subtype, stage)\n",
    "      - Image paths\n",
    "      - Tumor shape\n",
    "      - Literature (titles/journals/years)\n",
    "    \"\"\"\n",
    "\n",
    "    patient_id = context.get(\"patient_id\", \"\")\n",
    "    age = context.get(\"age\", \"\")\n",
    "    subtype = context.get(\"subtype\", \"\")\n",
    "    stage = context.get(\"stage_code\", \"\")\n",
    "\n",
    "    png_path = context.get(\"png_path\", \"\")\n",
    "    mask_path = context.get(\"mask_path\", \"\")\n",
    "    shape = context.get(\"shape\", \"\")\n",
    "    papers = context.get(\"papers\", [])\n",
    "\n",
    "    # Format literature text\n",
    "    lit_section = \"\\n\".join(\n",
    "        [f\"- {p['title']} ({p['journal']}, {p['year']})\" for p in papers]\n",
    "    )\n",
    "\n",
    "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are a radiology AI assistant. Generate a structured, clinically useful report based on the provided information.\n",
    "\n",
    "Basic Clinical Information:\n",
    "- Patient ID: {patient_id}\n",
    "- Age: {age}\n",
    "- Subtype: {subtype}\n",
    "- Stage Code: {stage}\n",
    "- Date of Report: {date_str}\n",
    "\n",
    "Image Information:\n",
    "- Original PNG: {png_path}\n",
    "- Segmentation Mask: {mask_path}\n",
    "- Predicted Tumor Shape: {shape}\n",
    "\n",
    "Instructions:\n",
    "Using all the information above (image, mask, shape, clinical data, and literature titles), generate a structured radiology-style report including:\n",
    "\n",
    "1. **Findings**\n",
    "   - Detailed description of tumor appearance in the *original PNG image*\n",
    "   - Clear explanation of what the *segmentation mask* highlights vs. misses\n",
    "   - Use precise radiology language (location, margins, enhancement pattern, architectural distortion, etc.)\n",
    "\n",
    "2. **Literature Context**\n",
    "   - Synthesize insights from the 20 provided paper titles\n",
    "   - Integrate findings with patient-specific factors (age, subtype, stage, tumor shape)\n",
    "   - Avoid quoting PMIDs or listing papers individually—provide an integrated discussion.\n",
    "\n",
    "3. **Suggested Next Clinical Steps**\n",
    "   - Provide actionable, realistic clinical recommendations\n",
    "   - Include biopsy, MDT consultation, receptor testing, genetic counseling, imaging follow-up, etc.\n",
    "   - Tailor recommendations to subtype (Luminal A), patient age, and shape characteristics.\n",
    "\n",
    "4. **Uncertainty / Limitations**\n",
    "   - Discuss segmentation accuracy limitations\n",
    "   - Single-slice imaging limitations\n",
    "   - Potential diagnostic uncertainty or areas needing further evaluation\n",
    "\n",
    "Write the report in paragraphs, concise and clinically oriented.\n",
    "\"\"\"\n",
    "\n",
    "    context[\"llm_prompt\"] = prompt\n",
    "    print(\"[Step5] LLM prompt constructed.\")\n",
    "    return {\"llm_prompt\": prompt}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e62310e1-7b52-4d38-89dc-2dd0ccc343ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step6_llm_summary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step6_llm_summary.py\n",
    "import google.generativeai as genai\n",
    "from config import GENAI_API_KEY, OUTPUT_DIR\n",
    "from pathlib import Path\n",
    "\n",
    "def run_step(context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Use Gemini-2.5-Flash to generate a final radiology report.\n",
    "    \"\"\"\n",
    "\n",
    "    if GENAI_API_KEY is None:\n",
    "        raise RuntimeError(\"GOOGLE_API_KEY not found\")\n",
    "\n",
    "    genai.configure(api_key=GENAI_API_KEY)\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "    prompt = context[\"llm_prompt\"]\n",
    "\n",
    "    print(\"[Step6] Calling Gemini-2.5-Flash Vision API...\")\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    summary = response.text\n",
    "\n",
    "    out_path = OUTPUT_DIR / f\"{context['patient_id']}_summary.txt\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(f\"[Step6] Summary saved to: {out_path}\")\n",
    "\n",
    "    return {\"summary_path\": str(out_path)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06591145-2af7-4964-bdea-b7fc93eea3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting step7_run_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile step7_run_agent.py\n",
    "\"\"\"\n",
    "Main LangChain-based workflow runner for DIP project.\n",
    "\n",
    "Pipeline:\n",
    "0) step0_prepare_case      - infer patient_id & GT .les path\n",
    "1) step1_dicom_to_png      - DICOM → PNG\n",
    "2) step2_medsam_segmentation - MedSAM segmentation\n",
    "3) step3_evaluation        - Dice / IoU with .les polygon\n",
    "4) step4_literature_search - PubMed: 'breast cancer'\n",
    "5) step5_build_prompt      - build LLM prompt\n",
    "6) step6_llm_summary       - call Gemini Vision to generate report\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "from config import DICOM_DIR\n",
    "from step0_prepare_case import run_step as step0\n",
    "from step1_dicom_to_png import run_step as step1\n",
    "from step2_medsam_segmentation import run_step as step2\n",
    "from step3_evaluation import run_step as step3\n",
    "from step4_literature_search import run_step as step4\n",
    "from step5_build_prompt import run_step as step5\n",
    "from step6_llm_summary import run_step as step6\n",
    "\n",
    "\n",
    "def _wrap_step(name, func):\n",
    "    \"\"\"Wrap step(context) into RunnableLambda.\"\"\"\n",
    "    def inner(context: dict):\n",
    "        print(f\"\\n===== Running {name} =====\")\n",
    "        result = func(context)\n",
    "        if result is None:\n",
    "            return context\n",
    "        context.update(result)\n",
    "        return context\n",
    "    return RunnableLambda(inner)\n",
    "\n",
    "\n",
    "def build_chain() -> RunnableSequence:\n",
    "    \"\"\"Build the LangChain workflow (sequence of steps).\"\"\"\n",
    "    chain = RunnableSequence(\n",
    "        _wrap_step(\"Step0: prepare case (patient_id & GT)\", step0),\n",
    "        _wrap_step(\"Step1: DICOM → PNG\", step1),\n",
    "        _wrap_step(\"Step2: MedSAM segmentation\", step2),\n",
    "        _wrap_step(\"Step3: Dice/IoU evaluation\", step3),\n",
    "        _wrap_step(\"Step4: PubMed 'breast cancer' search\", step4),\n",
    "        _wrap_step(\"Step5: build LLM prompt\", step5),\n",
    "        _wrap_step(\"Step6: Gemini Vision summary\", step6),\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def run_for_one_dicom(dicom_path: Path):\n",
    "    \"\"\"Complete the entire workflow for a single DICOM file\"\"\"\n",
    "    chain = build_chain()\n",
    "    context = {\"dicom_path\": str(dicom_path)}\n",
    "    final_context = chain.invoke(context)\n",
    "    return final_context\n",
    "\n",
    "\n",
    "def run_batch(test_mode: bool = True):\n",
    "    \"\"\"\n",
    "    test_mode = True  → Only run the first DICOM (for debugging)\n",
    "    test_mode = False → Run all DICOM files in the DICOM_DIR directory.\n",
    "    \"\"\"\n",
    "    dicom_files = sorted(\n",
    "        list(DICOM_DIR.glob(\"*.dcm\")) + list(DICOM_DIR.glob(\"*.dicom\"))\n",
    "    )\n",
    "\n",
    "    if not dicom_files:\n",
    "        print(f\"No DICOM files found in: {DICOM_DIR}\")\n",
    "        return\n",
    "\n",
    "    if test_mode:\n",
    "        dicom_files = dicom_files[:1]\n",
    "        print(\"[Runner] TEST_MODE = True, will only run 1 case.\")\n",
    "    else:\n",
    "        print(f\"[Runner] TEST_MODE = False, will run {len(dicom_files)} cases.\")\n",
    "\n",
    "    for dcm in dicom_files:\n",
    "        print(\"\\n=======================================\")\n",
    "        print(\"Running pipeline for:\", dcm.name)\n",
    "        print(\"=======================================\")\n",
    "        ctx = run_for_one_dicom(dcm)\n",
    "        print(\"\\n>>> Summary saved at:\", ctx.get(\"summary_path\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TEST_MODE = True\n",
    "    run_batch(test_mode=TEST_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a1ee15-c31f-47b1-9eb6-00a4c67df419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/shenyuyu/DIP Project/step7_run_agent.py\"\u001b[0m, line \u001b[35m21\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from step1_dicom_to_png import run_step as step1\n",
      "  File \u001b[35m\"/Users/shenyuyu/DIP Project/step1_dicom_to_png.py\"\u001b[0m, line \u001b[35m3\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    import pydicom\n",
      "\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'pydicom'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python step7_run_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f283bab2-23da-445e-b738-157885edc5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/dip_env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/envs/dip_env/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.18) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[Runner] TEST_MODE = True, will only run 1 case.\n",
      "\n",
      "=======================================\n",
      "Running pipeline for: TCGA-AO-A03M.dcm\n",
      "=======================================\n",
      "\n",
      "===== Running Step0: prepare case (patient_id & GT) =====\n",
      "[Step0] Found GT lesion file: /Users/shenyuyu/DIP Project/data/lesions/TCGA-AO-A03M.les\n",
      "[Step0] Clinical TSV loaded (1084 rows).\n",
      "[Step0] Clinical info — Age: 29, Subtype: BRCA_LumA, Stage: T1C_N0 (I-)_M0\n",
      "\n",
      "===== Running Step1: DICOM → PNG =====\n",
      "[Step1] Saved PNG: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M.png\n",
      "\n",
      "===== Running Step2: MedSAM segmentation =====\n",
      "[Step2] Running MedSAM segmentation...\n",
      "[Step2] Using device: cpu\n",
      "[Step2] Mask score = 0.4267, shape = Irregular\n",
      "[Step2] Saved mask: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_mask.png\n",
      "[Step2] Saved overlay: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_overlay.png\n",
      "\n",
      "===== Running Step3: Dice/IoU evaluation =====\n",
      "[Step3] Evaluating Dice/IoU with GT: /Users/shenyuyu/DIP Project/data/lesions/TCGA-AO-A03M.les\n",
      "[Step3] Dice = 0.0000, IoU = 0.0000\n",
      "[Step3] Saved visualization: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_pred_vs_gt.png\n",
      "\n",
      "===== Running Step4: PubMed 'breast cancer' search =====\n",
      "[Step4] Searching PubMed for: breast cancer Irregular\n",
      "[Step4] Retrieved 20 papers.\n",
      "\n",
      "===== Running Step5: build LLM prompt =====\n",
      "[Step5] LLM prompt constructed.\n",
      "\n",
      "===== Running Step6: Gemini Vision summary =====\n",
      "[Step6] Calling Gemini-2.5-Flash Vision API...\n",
      "[Step6] Summary saved to: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_summary.txt\n",
      "\n",
      ">>> Summary saved at: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} step7_run_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e54c1f-efb6-426e-aff0-6afd16cbfad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step2] Running MedSAM segmentation...\n",
      "[Step2] Using device: cpu\n",
      "[Step2] Mask score = 0.4453, shape = Irregular\n",
      "[Step2] Saved mask: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_mask.png\n",
      "[Step2] Saved overlay: /Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M_overlay.png\n"
     ]
    }
   ],
   "source": [
    "# 单独跑step2\n",
    "from step2_medsam_segmentation import run_step\n",
    "\n",
    "context = {\n",
    "    \"png_path\": \"/Users/shenyuyu/DIP Project/outputs/TCGA-AO-A03M.png\",\n",
    "    \"patient_id\": \"TCGA-AO-A03M\"\n",
    "}\n",
    "\n",
    "result = run_step(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f65ec3f4-2c08-4441-9fcc-c0e70f7f898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step3] No pred_mask in context → skip.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_vis_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverlay_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_vis_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_vis_path'"
     ]
    }
   ],
   "source": [
    "# run step 3\n",
    "context.update(run_step3(context))\n",
    "\n",
    "context[\"metrics\"]\n",
    "context[\"overlay_path\"]\n",
    "context[\"eval_vis_path\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022e206-f36f-47c5-9243-572f2bc8bfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dip_env)",
   "language": "python",
   "name": "dip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
